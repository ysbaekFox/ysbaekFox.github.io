---
published: true
layout: single
title: "[k8s] Kubernetes 중급편 정리"
category: k8s
tags:
comments: true
sidebar:
  nav: "mainMenu"
---
* * *

## Pod의 LifeCycle
**1) Pending 단계**
- 제일 먼저 Pod가 Node에 정상적으로 할당되면 **PodScheduled**가 true가 된다. (사용자가 특정 노드에 직접 할당을 하든지, 쿠버네티스가 자원 상황에 따라 자동으로 하든지)
- 이후에 Container가 실행되기전에 사전설정해주어야하는 것들이 있다면 실행되는 InitContainer가 Pending 단계 때 실행 된다. (볼륨 / 보안 셋팅 등)
- pod의 yaml 정의 spec에 initContainers라는 항목을 추가하여 실행 가능 함, 이것이 성공 적으로 수행 되면 **Initialized** 값이 true가 된다. (InitContainer가 없어도 true)
- 이후 Container의 이미지를 다운로드하는 동작이 수행 되고, 지금까지 설명한 단계동안 Container의 Status는 Waiting 상태이고, Reason은 ContainerCreating 임.

**2) Running**
- 만약 Pod에서 Continaer를 정상적으로 실행하지 못한 경우 Container의 상태는 Waiting이 되고 Reason은 CrashLoopBackOff가 됨.
- CrashLoopBackOff 상태에서 Container의 상태는 Ready로 판단하지만 내부적으로 ContainerReady 와 Ready는 False가 임.
- 이후 모든 컨테이너들이 정상적으로 기동이 되어 원할하게 돌아가게 되면 ContainerReady와 Ready는 False가 됨.
- 그러므로 상황에 따라 Pod의 상태 뿐만 이나리 Container의 상태도 모니터링 할 필요가 있음.
- 만약 Job이나 CronJob으로 생성된 Pod의 경우, 일을 수행하고 있을 때는 Running이지만 일을 수행하고 나면 Failed / Succeeded가 된다. 

**3) Failed**
- Failed / Succeded 결과에 상관 없이 ContainerReady와 Ready는 모두 False가 된다.
- Container가 수행을 완료하지 못한 경우 Terminated / Error가 된다.

**4) Succeeded**
- Failed / Succeded 결과에 상관 없이 ContainerReady와 Ready는 모두 False가 된다.
- Container가 수행을 완료한 경우 Terminated / Completed 된다.

**5) Unknown**
- Pending 중에 바로 Failed 혹은 Unkown 상태가 되는 경우도 있음.

## Pod - ReadinessProbe / LivenessProbe
- ReadinessProbe: 앱이 구동 되는 순간(부팅 과정)에 서비스와 연결 되자마자 전달 받은 Traffic에 대해 응답하지 못해 발생하는 문제를 해결하기 위한 기능.
- LivenessProbe: K8S 서비스와 적접적으로 연결된 Container에는 문제가 없으나 컨테이너 내부의 구동 중인 App에 문제가 생긴 경우 Service가 문제를 감지하여 Traffic 실패를 해결하기 위한 기능. 

**사용 방법**
- 사용 목적이 다를 뿐 설정할 수 있는 내용은 ReadinessProbe와 LivenessProbe가 같습니다.
- httpGet / Exec / tcpSocket 이라는 필수 옵션이 있음.
- initialDealySeconds / periodSeconds / timeoutSeconds / successThreshold / failureThreshold 라는 선택 옵션이 있음. 각각 0초 / 10초 / 1초 / 1회 / 3회 의 default 값을 가짐.
- Pod가 Running이 되더라도 Probe의 조건을 만족하지 못할 경우 ContainerReady와 Ready가 false에서 true로 변경되지 않음.

## ReadinessProbe / LivenessProbe 실습
<br>

**1) ReadinessProbe 실습**

<details>
<summary> yaml 펼치기 </summary>
<div markdown="1">

```yaml
# Service 정의
apiVersion: v1
kind: Service
metadata:
  name: svc-readiness
spec:
  selector:
    app: readiness
  ports:
  - port: 8080
    targetPort: 8080
---
# 일반 Pod 정의
apiVersion: v1
kind: Pod
metadata:
  name: pod1
  labels:
    app: readiness  
spec:
  containers:
  - name: container
    image: kubetm/app
    ports:
    - containerPort: 8080	
  terminationGracePeriodSeconds: 0
---
# rediness Pod 정의
apiVersion: v1
kind: Pod
metadata:
  name: pod-readiness-exec1
  labels:
    app: readiness  
spec:
  containers:
  - name: readiness
    image: kubetm/app
    ports:
    - containerPort: 8080	
    readinessProbe:
      exec:
        command: ["cat", "/readiness/ready.txt"]
      initialDelaySeconds: 5
      periodSeconds: 10
      successThreshold: 3
    volumeMounts:
    - name: host-path
      mountPath: /readiness
  volumes:
  - name : host-path
    hostPath:
      path: /tmp/readiness
      type: DirectoryOrCreate
  terminationGracePeriodSeconds: 0
```

</div>
</details>
<br>
아래와 같이 일반 pod와 readiness pod를 실행한 후 요청을 보내면 readiness pod로부터는 응답이 없는 것을 확인할 수 있습니다. (ready.txt 파일이 없으므로)

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/01b115f3-96d9-45ef-a8b1-196ae180e470)

몇가지 커맨드를 입력하여 pod의 상태를 확인해보도록 하겠습니다.

```shell
$ kubectl get events -w | grep pod-readiness-exec1
$ kubectl describe pod pod-readiness-exec1 | grep -A5 Conditions
$ kubectl describe endpoints svc-readiness
```

우선 ready.txt 파일을 찾을 수 없구요, Conditions를 보면 ContainersReady와 Ready 값이 false 입니다. 
그리고 NotReadyAddresses를 확인해보면 readiness-pod의 IP를 확인할 수도 있네요.

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/ea3934cc-5eb8-479e-b8b2-203dc0f3fb76)

이후에 readiness pod가 실행 중인 node에 접속하여 ready.txt 파일을 생성해주니 정상적으로 응답하는 것을 확인할 수 있었습니다.

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/045b34c8-19af-43c3-93e7-d04272135293)

<br>

**2) LivenessProbe 실습**

LivenessProbe 실습을 해보겠습니다. livenessProbe를 가지는 Pod를 생성하고 테스트해보도록 하겠습니다.

<details>
<summary> yaml 펼치기 </summary>
<div markdown="1">

```yaml
apiVersion: v1
kind: Service
metadata:
  name: svc-liveness
spec:
  selector:
    app: liveness
  ports:
  - port: 8080
    targetPort: 8080
---
apiVersion: v1
kind: Pod
metadata:
  name: pod2
  labels:
    app: liveness
spec:
  containers:
  - name: container
    image: kubetm/app
    ports:
    - containerPort: 8080
  terminationGracePeriodSeconds: 0
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-liveness-httpget1
  labels:
    app: liveness
spec:
  containers:
  - name: liveness
    image: kubetm/app
    ports:
    - containerPort: 8080
    livenessProbe:
      httpGet:
        path: /health
        port: 8080
      initialDelaySeconds: 5
      periodSeconds: 10
      failureThreshold: 3
  terminationGracePeriodSeconds: 0
```
</div>
</details>
<br>
현재 LivenessProbe가 정상적으로 동작 중인 것을 확인할 수 있습니다.

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/9073061b-a468-438b-9688-abb438edf87a)

이번엔 강제로 status 500을 전달하여 pod의 상태가 어떻게 변화하는지 확인해보겠습니다. status500을 3번 전달하니 LivenessProbe가 Container에 이상이 생겼음을 감지하고 Container를 재시작하는 것을 확인할 수 있었습니다.

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/b84dabe5-65cb-4b04-8e9e-af351ddfd928)

## QoS classes (Guaranteed / Burstable / BestEffort)

Node에 균등하게 자원을 사용하는 Pod가 여러개 동작하고 있다고 가정할 떄, 
그 중 한개의 Pod가 자원을 추가적으로 사용해야하는 상황에서 추가 자원이 없는 경우 어떻게 동작할지에 대해 정의하는 기능 입니다. 쿠버네티스에서는 앱의 중요도에 따라 이것을 Control할 수 있도록 QoS라는 기능을 제공하고 있습니다. 그러한 Pod들은 중요도 순으로 BestEffort < Burstable < Guranteed로 구분 됩니다.  

특이한 점은 따로 QoS 구분을 위한 정의부가 있는 것이 아니라 spec의 resources / requetes / limits이 정의가 어떻게 되어 있냐에 따라 자동으로 설정 됩니다.

**1) Guaranteed**
- Pod의 모든 Container에 Request와 Limit가 설정되어 있어야 한다.
- Request와 Limit에는 Memory와 CPU가 모두 설정되어 있어야 한다.
- 각 Container 내의 Memory와 CPU의 Reuqest와 Limit의 값이 같아야 한다.

**2) Burstable**
- Guaranteed / BestEffort가 아닌 모든 경우
- 그렇다면 같은 Burstable Pod 간에는 OOM Score가 더 높은 것이 먼저 삭제 된다. (OOM Score는 Request Memory 대비 실제 App의 Memory 사용량)

**3) BestEffort**
- 어떤 Container 내에도 Request와 Limit이 설정되지 않은 경우.

## Node Scheduling
- NodeName: 노드의 이름으로 Pod를 할당할 수 있음. 실제 Production 환경에서는 노드가 삭제되고 추가되면서 노드의 이름이 변경될 수 있기 때문에 범용설 떨어짐.
- NodeSelector: 가장 많이 사용되는 방법으로 key / label을 추가하면 해당 label이 달려있는 Node에 할당할 수 있음. 자원이 없어도 label에 맞는 Node에만 할당하려고 하는 점이 단점.
- NodeAffinity: Pod에 key만 설정하면 특정 Node에 할당 시킬 수 있음. 만약 자원이 없는 경우에 조건에 따라 할당하도록 하는 옵션도 있음.
- Pod Affinity: Pod를 Node에 할당할 때 집중하는 것을 지원하는 기능, 특정 2개의 Pod가 hostPath를 공유하는 상황에서 2개의 Pod는 같은 Node에 할당되어야만 하는데 이러한 상황에서 유용.
- Pod Anti-Affinity: Pod를 Node에 할당할 때 분산하는 것을 지원하는 기능, 특정 2개의 Pod 중 1개의 Pod에 문제가 생겼을 때 나머지 Pod가 문제를 기록해야하는 상황에서 유용. (만약 2개의 Pod가 같은 Node에 있고 Node에 문제가 
생겨 2개의 Pod 모두에 문제가 생기면 안되므로)
- Taint / Toleration: 특정 Node에 아무 Pod나 할당되지 않도록 제한하는 용도, Node가 하나 있고 해당 Node는 고성능 그래픽 자원을 사용하는데에 필요한 Node일 경우 그래픽 자원이 필요한 Pod만 할당되도록 설정할 수 있음. Taint라는 옵션이 붙은 Node에는 Toleration이라고 옵션이 붙은 Pod만 할당 될 수 있음. 참고로 Taint Node에는 Toleration이 아닌 Pod가 자동으로 할당 안될 뿐 아니라 Node를 직접 지정하는 방법으로도 할당 될 수 없음. **여기서 중요한 점은 Pod가 Taint Node를 찾아서 할당하는 것이 아니라 Node에 할당되는 순간에 Toleration인지를 판단하는 것이므로 Taint Node에 할당될 수 있도록 Node Scheduling을 직접해주어야만 함.**

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/de45f574-c2e3-4bdb-a9ab-d59893cae664)

## Node Affinity 실습

**1) matchExpressions Exists operator 실습**

worker-1 노드에는 kr=az-1, worker-2 노드에는 us=az-1 Label을 붙인 뒤, key가 kr인 Label이 있는 Node에 Pod를 생성해보도록 하겠습니다.

```
kubectl label nodes worker-1 kr=az-1
kubectl label nodes worker-2 us=az-1
```

```yaml
apiVersion: v1
kind: Pod
metadata:
 name: pod-match-expressions1
spec:
 affinity:
  nodeAffinity:
   requiredDuringSchedulingIgnoredDuringExecution:   
    nodeSelectorTerms:
    - matchExpressions:
      -  {key: kr, operator: Exists}
 containers:
 - name: container
   image: kubetm/app
 terminationGracePeriodSeconds: 0
```

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/71a904b6-8f1f-489d-93b9-e586b94ef2f8)

**2) Required 실습**

Required는 반드시 조건을 만족하는 Node에서만 생성 됩니다. key: ch인 label을 가지는 Node가 없으므로 Pending 상태에서 넘어가지 못합니다.

```yaml
apiVersion: v1
kind: Pod
metadata:
 name: pod-required
spec:
 affinity:
  nodeAffinity:
   requiredDuringSchedulingIgnoredDuringExecution:
    nodeSelectorTerms:
    - matchExpressions:
      - {key: ch, operator: Exists}
 containers:
 - name: container
   image: kubetm/app
 terminationGracePeriodSeconds: 0
```

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/f5eabb1b-c6a3-4d93-a380-338f5923c5a5)

**2) Preferred 실습**

현재 ch key를 가지는 Node가 존재하지 않습니다. Preferred의 경우 반드시 조건을 만족해야만 생성하는 것이 아니라 뜻 그대로 '선호'정도의 강제성입니다. 
그러므로 worker-1에는  pod-match-expressions1 pod가 있으므로 worker-2에 할당이 될 것 입니다. (상황에 따라 아닐 수도 있음.)

```yaml
apiVersion: v1
kind: Pod
metadata:
 name: pod-preferred
spec:
 affinity:
  nodeAffinity:
   preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 1 # 여러개의 weight 설정할 수 있고, 
                # weight를 설정해서 여러개의 옵션에 대한 중요도 설정이 가능.
      preference:
       matchExpressions:
       - {key: ch, operator: Exists}
 containers:
 - name: container
   image: kubetm/app
 terminationGracePeriodSeconds: 0
```

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/5b29368b-9f16-46cf-8aa9-cb407a564403)

## Pod - Affinity / Anti-Affinity 실습

**1) Web1 Pod 실습**

먼저, worker-1 / worker-2 node에 라벨링 작업을 수행합니다.

```
kubectl label nodes worker-1 a-team=1
kubectl label nodes worker-2 a-team=2
```

```yaml
apiVersion: v1
kind: Pod
metadata:
 name: web1
 labels:
  type: web1
spec:
 nodeSelector:
  a-team: '1'
 containers:
 - name: container
   image: kubetm/app
 terminationGracePeriodSeconds: 0
```
a-team=1인 node는 worker-1이므로  worker-1에 할당 됩니다. 
이 Pod에는 type: web1 label이 설정되어 있는데, Pod Affinity 기능 실습에 다시 사용할 예정입니다.

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/93290533-201a-4b41-a1ab-c48d2f75c16b)

**2) Web1 Affinity Pod 실습**

server1이라는 pod를 web1이라는 pod가 존재하는 node에 생성하는 기능을 실습해보겠습니다

```yaml
apiVersion: v1
kind: Pod
metadata:
 name: server1
spec:
 affinity:
  podAffinity:
   requiredDuringSchedulingIgnoredDuringExecution:   
   - topologyKey: a-team
     labelSelector:
      matchExpressions:
      -  {key: type, operator: In, values: [web1]}
 containers:
 - name: container
   image: kubetm/app
 terminationGracePeriodSeconds: 0
```

server1 pod가 web1 pod가 존재하는 worker-1 node에 생성된 것을 확인할 수 있습니다.

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/da14dbfb-1d98-4556-b149-e84e4fcfe024)

**3) Web2 Affinity Pod 실습**

이번엔 web2 pod가 존재하는 node에 server2 pod를 생성하는 예제를 실행해보겠습니다.

```yaml
apiVersion: v1
kind: Pod
metadata:
 name: server2
spec:
 affinity:
  podAffinity:
   requiredDuringSchedulingIgnoredDuringExecution:   
   - topologyKey: a-team
     labelSelector:
      matchExpressions:
      -  {key: type, operator: In, values: [web2]}
 containers:
 - name: container
   image: kubetm/app
 terminationGracePeriodSeconds: 0
```

현재 web2 pod는 존재하지 않습니다, 그래서 server2 pod는 생성되지 않고 pending 상태에서 대기하게 됩니다.

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/6c981327-f52e-4f46-880c-741fb518e03e)

**4) Web2 Pod 실습**

그렇다면 server2 pod가 pending된 상태에서 web2 pod를 생성하면 어떻게 될까요?

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/9677db73-dea3-4b50-9ec4-c5dd48c56fca)

**5) Anti-Affinity Pod 실습**

Pod Anti-Affinity 기능을 실습해보겠습니다. master pod를 생성하고, 
slave pod를 생성할 때 podAntiAffinity를 master pod로 설정하여 동일한 Node에 생성되지 않는지 확인해보겠습니다.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: master
  labels:
     type: master
spec:
  nodeSelector:
    a-team: '1'
  containers:
  - name: container
    image: kubetm/app
  terminationGracePeriodSeconds: 0
---
apiVersion: v1
kind: Pod
metadata:
 name: slave
spec:
 affinity:
  podAntiAffinity:
   requiredDuringSchedulingIgnoredDuringExecution:   
   - topologyKey: a-team
     labelSelector:
      matchExpressions:
      -  {key: type, operator: In, values: [master]}
 containers:
 - name: container
   image: kubetm/app
 terminationGracePeriodSeconds: 0
```

서로 다른 node에 생성되는 것을 확인 할 수 있습니다.

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/cbf2fff1-9807-460d-bc14-d2d511a0628f)

## Taint / Toleration 실습

Taint에 대한 설명을 좀 더 보충하였습니다.
- "Taint"는 노드에 적용되는 제약 조건으로서, 특정 리소스를 사용할 수 없게 하거나 특정 용도로 제한하는 등의 제어를 가능하게 함. 
- 일반적으로는 클러스터 관리자가 특정 노드에 Taint를 설정하여 해당 노드에 특정 유형의 Pod가 스케줄링되지 않도록 하는 등의 용도로 사용됨.

```
kubectl label nodes worker-1 gpu=no1
kubectl taint nodes worker-1 hw=gpu:NoSchedule
```

즉, "NoSchedule" Taint가 설정된 노드에는 해당 Taint와 일치하는 Tolerations(톨러레이션)이 없는 한 Pod가 스케줄링되지 않음.

**1) Pod without Toleration 실습**

아래 pod는 node selector를 사용하여, worker-1에 pod를 생성하게 합니다. 그런데 worker-1에는 현재 taint가 설정되어 있는데 pod에는 Toleration이 없으므로 pending 상태가 됩니다.

```yaml
apiVersion: v1
kind: Pod
metadata:
 name: pod-without-toleration
spec:
 nodeSelector:
  gpu: no1
 containers:
 - name: container
   image: kubetm/app
 terminationGracePeriodSeconds: 0
```

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/31332e95-3e1b-491a-968c-599c6185d776)

**2) Pod with Toleration 실습**

이번엔 toleration 옵션을 넣은 pod를 생성해보도록 하겠습니다, 이번에는 정상적으로 생성 되는 것을 확인할 수 있습니다.


```yaml
apiVersion: v1
kind: Pod
metadata:
 name: pod-with-toleration
spec:
 nodeSelector:
  gpu: no1
 tolerations:
 - effect: NoSchedule
   key: hw
   operator: Equal
   value: gpu
 containers:
 - name: container
   image: kubetm/app
 terminationGracePeriodSeconds: 0
 ```

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/f198ced5-8d7c-4cec-8f18-4ff04dd2c357)


**3) Pod1 with NoExecute 실습**

Taint / Toleration 에서 지원하는 옵션에는 NoSchedule 외에도 PreferNoSchedule / NoExecute / Custom Taint 등이 있습니다.

그 중 NoExecute를 추가로 실습해보도록 하겠습니다. 참고로  Taint의 NoSchedule 기능은 기존에 Node에서 동작 중인 Pod에는 영향이 가지 않는데 NoExecute는 영향이 갑니다.
NoExecute에 대해서 추가로 테스트 해보도록 하겠습니다. 

먼저 아래와 같은 Pod를 생성하면, worker-1에는 taint가 있기 때문에 무조건 worker-2에 할당됩니다.

```yaml
apiVersion: v1
kind: Pod
metadata:
 name: pod1-with-no-execute
spec:
 tolerations:
 - effect: NoExecute
   key: out-of-disk
   operator: Exists
   tolerationSeconds: 30
 containers:
 - name: container
   image: kubetm/app
 terminationGracePeriodSeconds: 0
```

그리고, 바로 이어서 tolerationSeconds가 없는 pod2를 하나 더 생성해보도록 하겠습니다.

```yaml
apiVersion: v1
kind: Pod
metadata:
 name: pod2-without-no-execute
spec:
 containers:
 - name: container
   image: kubetm/app
 terminationGracePeriodSeconds: 0
```

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/2541af0c-f36c-45dc-bbb3-edb6a0136e52)

그리고 2개의 pod가 생성된 상태에서, 기존에 worker-1에 달았던 taint를 삭제하고, worker2에는 새로운 taint인 out-of-disk=True:NoExecute를 추가한 뒤 결과를 살펴보겠습니다.

```shell
# worker-1에 NoSchedule를 삭제해주지 않으면, worker-2에도 taint가 있고
# 그 어떤 곳도 taint가 없는 곳이 없어서 문제 발생.
kubectl taint nodes worker-1 hw=gpu:NoSchedule-
kubectl taint nodes worker-2 out-of-disk=True:NoExecute
```

worker-2에 taint를 부여한 뒤, pod2는 tolerationSeconds가 없으므로 곧바로 삭제 됩니다.

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/96670fad-e6f2-4028-8926-63ba587a3607)

이후 pod1은 tolerationSeconds가 30이므로 30초 뒤에 삭제되는 것을 확인할 수 있습니다.

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/fc8dd756-be41-42c9-9ca7-be6be2aff268)