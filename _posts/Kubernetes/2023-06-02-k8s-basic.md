---
published: true
layout: single
title: "[k8s] Kubernetes Study Summary"
category: k8s
tags:
comments: true
sidebar:
  nav: "mainMenu"
---
* * *

쿠버네티스 Study 및 실습한 내용 정리한 페이지 입니다. (참고로 클러스터는 3개의 Node를 VBox Ubuntu 가상환경으로 구성하였습니다.)

* * *
# Node(노드)
- Kubernetes는 Container를 Pod 내에 배치하고 Node에서 실행함으로 워크로드를 구동함.
- Node는 구성한 Cluster에 따라 가상 또는 물리적 머신일 수 있음. 
- 각 노드는 control-plane(node 중 하나)에 의해 관리되며 Pod를 실행하는 데 필요한 Service를 포함한다.
- Node의 구성 방법에 대해서는 따로 기재하지 않음.

## 실습
```
kubectl get node %namespace%
```
<br>
![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/0dddae4e-c822-4379-baa9-67e6b755a212)

```
kubectl describe node %node_name%
```
![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/3b438b7e-7c89-43bf-9746-d386492819f0)

<br>

# Pod(파드)
* * *

## Pod
- Pod의 안에는 Container들이 존재
- 각각의 Container들은 Service가 연결될 수 있도록 Port를 가지고 있다.
- 한 Cotainer는 하나 이상의 Port를 가질 수 있지만, Container간에 중복되는 Port는 가질 수 없다.
- Pod내의 Container들은 하나의 Host로 묶여있다, 그래서 다른 Container에 접근할 때 localhost를 사용할 수 있다.
- Pod가 생성 될 때 고유한 IP가 자동으로 할당 되는데, 해당 IP는 클러스터 내부에서만 접근 가능하다. 그리고 Pod가 재생성 될 경우 IP 주소는 변경된다.

## Label
- Label은 Pod 뿐만아니라 모든 오브젝트에 추가할 수 있음 (Pod에서 가장 많이 사용)
- 오브젝트들을 목적에 따라 분류하고 분류한 오브젝트들만 따로 연결하기 위해서 사용
- Label은 key/value가 한쌍 ex) type:wep, lo:dev, type:db, lo:prod

## Node Scheldule
- Pod는 특정 Node에 할당되어야 하는데, 자동으로 할당 되는 것과 직접 스케쥴링하는 방법이 있음.
- 직접 Node를 할당하는 경우, Node에 Label을 달아주고, Pod를 생성할 때 nodeSelector 항목에 Node의 Label과 매칭되는 key value를 넣어주면 됨.
- 자동으로 Node를 할당하는 경우 yaml에 memory 등을 제한하는 항목을 추가하면 자원 상태에 따라 스케줄러가 판단하여 자동으로 Node에 할당한다.

## yaml 예시

**1) Pod 생성**
- container1/container2라는 pod를 생성한다.
- image는 각각 kubetm/p8000와 kubetm/p8080을 사용한다.
- containerPort는 각각 8000과 8080을 사용한다.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-1
spec:
  containers:
  - name: container1
    image: kubetm/p8000
    ports:
    - containerPort: 8000
  - name: container2
    image: kubetm/p8080
    ports:
    - containerPort: 8080
```

**2) label 추가하여 Pod 생성**
- type: web, lo: dev로 label 추가하여 pod 생성

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-2
  labels:
    type: web
    lo: dev
spec:
  containers:
  - name: container
    image: kubetm/init
```

**3) Node Schedule 사용하여 Pod 생성**
- worker-1 node에 pod 생성
- worker-1에 pod를 생성할 자원이 충분하지 않으면, pending 상태에서 넘어가지 못함.
- request는 Container가 생성될 때 노드에 요청하는 리소스의 양, limit은 해당 컨테이너가 생성된 후에 사용할 수 있는 리소스의 최대치
- Container 가동 중, 메모리 사용량이 limit을 넘어가면 재구동됨 (Cpu 사용량의 경우 limit을 넘으면 Pod를 재가동 시키진 않고,  request까지 낮추어서 퍼포먼스 저하시킴)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-3
spec:
  nodeSelector:
    kubernetes.io/hostname: worker-1
  containers:
  - name: container
    image: kubetm/init
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-4
spec:
  containers:
  - name: container
    image: kubetm/init
    resources:
      requests:
        memory: 2Gi
      limits:
        memory: 3Gi
```

#### Command

```
# 둘다 자원을 생성할때 사용할 수 있지만
# create는 기존에 같은 이름의 Pod가 존재하면 생성이 안되고,  
# apply는 기존에 같은 이름의 Pod가 존재하면 업데이트됨.
kubectl create -f %yaml_file%
kubectl apply -f %yaml_file%
kubectl delete -f %yaml_file%
```

```
kubectl describe pod %pod_name% -n %namespace%
kubectl describe -f %yaml_file%
```

```
kubectl get pods -n %namespace% -o wide
kubectl get pod %pod_name% (default)
kubectl get pod %pod_name% -n %namespace% (specific namespace)
kubectl get pod %pod_name% -A (all namespace)
kubectl get pod %pod_name% --all-namespaces=true (all namespace)
```

```
# -i: --stdin 컨테이너에 표준 입력을 연결, 컨테이너 내부에서 사용자의 입력을 받을 수 있음.
# it: --tty 컨테이너에 터미널을 할당, 컨테이너 내부에서 터미널과 상호작용할 수 있음.
kubectl exec -i -t %pod_name% -c %container_name% -- /bin/bash
```

<br>

# Service(서비스)
* * *

## ClusterIP (Default)
- Service는 본인의 ClusterIP를 가지고 있다.
- Service의 ClusterIP를 사용하여 Service에 연결된 Pod에 접근할 수 있다. (Pod는 재생성 가능하기 때문에 Cluster 내부에서 접근가능한 Pod의 IP는 변경 될 수 있음)
- Service는 사용자가 조작하지 않는 이상 클러스터가 스스로 삭제하거나 재생성하지 않는다. 그러므로 Service의 ClusterIP는 불변하다.
- Service의 ClusterIP도 Pod와 마찬가지로 Cluster 내부에서만 접근 가능하다.
- Service에는 여러 개의 Pod를 연결할 수 있다.
- Service에 여러 개의 Pod가 연결된 경우, Service가 트래픽을 분산시킨다.
- 인가된 사용자, 내부 대시보드, Pod의 서비스상태 디버깅 등에 사용.

## 실습

**1) Service 정보**
```
kubectl get services -o wide
kubectl get service %service_nam% -n %namespace%
```
![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/40d2db6a-4fca-4074-924c-94c0b6f66cf8)

**2) Service 실행**
- worker-1 node에 kubetm/app image로 containerPort 8080, 8081을 가지는 container 생성 
- 생성한 pod는 label에 key/value 값 app: pod을 가짐.
- 실행한 service는 selector 사용하여 label key/value에 app: pod를 가지는 pod에 대해 적용
- 이 서비스의 9000번 포트로 들어온 요청은 백엔드 파드의 8080번 포트로 전달 됨.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-1
  labels:
     app: pod
spec:
  nodeSelector:
    kubernetes.io/hostname: worker-1
  containers:
  - name: container
    image: kubetm/app
    ports:
    - containerPort: 8080
    - containerPort: 8081
```

```yaml
apiVersion: v1
kind: Service
metadata:
  name: svc-1
spec:
  selector:
    app: pod
  ports:
  - port: 9000
    targetPort: 8080
  - port: 9090
    targetPort: 8080
```

**3) Service ClusterIP 사용하여 Pod에 Request 보내기**
```
curl {ClusterIP}:{port}/hostname
```
![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/e39538b2-9c75-4133-b53b-b42768b66722)

## NodePort
- NodePort로 생성된 Service에는 기본적으로 ClusterIP를 가진다.
- NodePort로 Service를 생성하면, Cluster에 생성된 모든 노드에서 동일한 NodePort 1개로 접근할 수 있다.
- 참고로 Service와 연결된 Pod가 존재하는 Node에만 NodePort가 할당되는 것이 아니라 모든 Node에 Port가 할당 된다.
- NodePort로 생성된 서비스는 노드에 상관 없이 트래픽을 분산 시킨다.
- externalTrafficPolicy라는 옵션을 사용하면, 특정 Node에서 받은 Traffic은 해당 Node로만 전달하도록 할 수 있다.
- 내부망 연결, 데모/임시 연결용.

## 실습

#### externalTrafficPolicy 옵션
- Cluster (Default): 트래픽이 전달된 노드 내부의 Pod외에 다른 노드의 Pod로도 전달될 수 있음. (Node 단위의 로드 밸런싱 O)
- Local: 트래픽이 전달된 노드 내부의 Pod로만 전달될 수 있음(Node 단위의 로드 밸런싱 X)

```yaml
# port 30000은 Node의 모든 Pod에 할당 됩니다.
# Node의 IP (Internal 혹은 External)를 사용하여 Port에 접근 가능합니다.

apiVersion: v1
kind: Service
metadata:
  name: svc-2
spec:
  selector:
    app: pod
  ports:
  - port: 9000
    targetPort: 8080
    nodePort: 30000
  type: NodePort
  externalTrafficPolicy: Local
```

#### NodePort로 curl 요청 보내기
- pod가 실행 중인 node의 IP로 nodePort 사용하여 요청

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/00cdaf79-346c-4236-87a3-a72337d662f6)

<br>

## Load Balancer
- Load Balancer 생성된 Service에는 기본적으로 ClusterIP를 가진다.
- Load Balancer로 생성된 Service의 외부 접속 IP는 자동으로 생성되지 않으므로 외부 접속을 위한 플러그인을 설치해주어야 한다. (클라우드 서비스 제외)
- 외부에 시스템 노출용.

## 실습
- 해당하는 label로 load balancing 자동 수행

```yaml
apiVersion: v1
kind: Service
metadata:
  name: svc-3
spec:
  selector:
    app: pod
  ports:
  - port: 9000
    targetPort: 8080
  type: LoadBalancer
```

<br>

# Volume(볼륨)
* * *

## emptyDir
- 컨테이너들끼리 데이터를 공유하기 위해서 Volume을 사용하는 것, 최초 Volume이 생성 될 때는 비어있기 때문에 emptyDir이라고 명명.
- Pod안의 Container들은 Volume 공간의 데이터에 자유롭게 접근할 수 있음.
- Volume은 Pod안에 생성되기 때문에, Pod 생성 시에 만들어지고 삭제시에 없어짐, 그러므로 영속성이 필요하지 않은 임시 데이터에 대해서 사용해야 함.

## 실습
- container1 접속시 mount2는 확인 할 수 없음. volume은 리소스는 공유하지만 격리 되어 있음.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-volume-1
spec:
  containers:
  - name: container1
    image: kubetm/init
    volumeMounts:
    - name: empty-dir
      mountPath: /mount1
  - name: container2
    image: kubetm/init
    volumeMounts:
    - name: empty-dir
      mountPath: /mount2
  volumes:
  - name : empty-dir
    emptyDir: {}
```

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/97e27d86-52a3-4bf4-ab2b-b63e31fe9205)

<br>

## hostPath
- emptyDir와 달리 경로를 Pod가 아닌 Node에서 공유함.
- Pod가 재생성 될 때, 혹은 Node에 문제가 생겨 Pod가 옮겨질 때 본래의 Node에서 Pod가 재생성되리라는 보장이 없으므로 고려해야 함.
- 사용자가 직접 hostPath의 경로를 Node 추가시마다 Mount를 걸어줌으로서, Pod가 다른 Node로 옮겨지더라도 문제가 발생하지 않도록 처리할 수 있음. (추천 X)
- hostPath는 Pod가 생성 되기 전에, 경로가 생성되어 있어야 오류가 발생하지 않음.  
- Pod의 Data를 저장하기 위함이 아니라, Pod가 할당 되어 있는 host(Node)의 Data를 사용해야할 때 적절할 수 있음.

## 실습
- container 접근 시 mount1 경로 확인 됨.
- worker-1 노드 접근 시 node-v 경로 확인 됨.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-volume-3
spec:
  nodeSelector:
    kubernetes.io/hostname: worker-1
  containers:
  - name: container
    image: kubetm/init
    volumeMounts:
    - name: host-path
      mountPath: /mount1
  volumes:
  - name : host-path
    hostPath:
      path: /node-v
      type: DirectoryOrCreate
```

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/db2277e5-2202-4c47-a6c4-b20ca9deed7f)
  
![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/7ff771e6-f908-485a-89f0-dc9e236ba9c2)

<br>

## PVC(Persistent Volue Claim) / PV(Persistent Volume)
- Local Storage 혹은 NFS, AWS, git 등의 PV와 연결하여 Pod에 영속성이 있는 Data 영역을 제공하기 위함.
- Kubernetes에서는 PVC / PV 를 사용할 때 User영역과 Admin 영역으로 분류하였음.
- Admin이 PV를 생성한 후에, User가 PVC를 생성하면 Kubernetes가 PVC에 맞는 적절한 PV를 연결함. 이후 Pod를 만들 때 PVC를 마운팅하여 사용.

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/9e62f167-f9f5-4a88-92d3-60747c098619)

## 실습

**1) PV-PVC 연결하기**
- PersistentVolume을 local로 생성, Node 경로에 /node-v 생성 됨.
- PersistentVolumeClaim을 생성하면 k8s가 적절한 PV를 연결함, 아래에서는 PVC가 요청하는 storage 사이즈가 1G이므로 이미 생성되어져있던 pv-03의 storage 2G에 할당됨.
- 이후, Pod 생성시 pvc-pv라는 이름으로 volume mount를 해주고 해당 이름에 대한 persistentVolumeClaim을 연결해주면 됨.

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-03
spec:
  capacity:
    storage: 2G
  accessModes:
  - ReadWriteOnce
  local:
    path: /node-v
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - {key: kubernetes.io/hostname, operator: In, values: [worker-1]}
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-01
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1G
  storageClassName: ""
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-volume-3
spec:
  containers:
  - name: container
    image: kubetm/init
    volumeMounts:
    - name: pvc-pv
      mountPath: /mount3
  volumes:
  - name : pvc-pv
    persistentVolumeClaim:
      claimName: pvc-01
```

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/1c414971-f44e-40a9-83a8-76aeb578dce8)

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/e76c65c6-b63c-4b9e-b8d4-0cd34c10e651)

**2) PV-PVC를 label과 selector를 이용해 연결하기**
- PVC를 label을 사용하여 특정 PV를 명시적으로 지정하여 연결하는 방법.

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-04
  labels:
    pv: pv-04
spec:
  capacity:
    storage: 2G
  accessModes:
  - ReadWriteOnce
  local:
    path: /node-v
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - {key: kubernetes.io/hostname, operator: In, values: [worker-1]}
```

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-04
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 2G
  storageClassName: ""
  selector:
    matchLabels:
      pv: pv-04
```

# ConfigMap(컨피그맵) / Secret(시크릿)
* * *

## ConfigMap, Secret
- 외부에서 관리할 수 있는 간단한 값의 차이만 존재하는 두 개의 이미지를 하나의 이미지로 관리하기 위한 기능
- ConfigMap과 Secret을 생성하여 Pod 생성 시 연결할 수 있음, Pod 생성 시 해당 값들은 환경 변수에 추가 됨.
- ConfigMap과 Secret의 차이는 Secret은 보안 데이터를 저장하는데 사용. (Dashboard는 보안상의 이유로 실제로 잘 사용하지 않음.)

## 실습

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cm-dev
data:
  SSH: 'false'
  User: dev
---
apiVersion: v1
kind: Secret
metadata:
  name: sec-dev
data:
  Key: MTIzNA==
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-1
spec:
  containers:
  - name: container
    image: kubetm/init
    envFrom:
    - configMapRef:
        name: cm-dev
    - secretRef:
        name: sec-dev
```

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/c77e406f-e405-4be8-b3ba-7fcd20ea2c10)

## Env (Literal)
- 상수를 넣는 방법
- Secret은 Base64 인코딩을한 값을 넣어야 함. (이것이 Secret의 보안 규칙은 아님, 단지 Secret의 Value 규칙)
- Pod로 주입될 때는 자동으로 디코딩 되어서 환경변수에서는 본래의 값을 볼 수 있음.
- Secret은 메모리에 저장되고, 최대 1Mbyte까지만 넣을 수 있음. (메모리 사용하므로 성능에 주의 필요) 

## 실습

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/9f8e193f-cff2-42e8-9796-e9af069b5464)

## Env (File)
- file로 configmap을 만드는 것은 dashboard에서 지원하지 않으므로 명령어로 수행해야 함.
- 예시) kubectl createconfigmap cm-file --from-file=./file.txt, kubectl create secret generic sec-file --from-file=./file.txt

## 실습

```
# file-c.txt
--------------
Content
--------------
# file-s.txt
--------------
Content
--------------
```
```
kubectl create configmap cm-file --from-file=./file-c.txt
kubectl create secret generic sec-file --from-file=./file-s.txt
```

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/79c27209-abb3-47a1-abe9-2a769ffd5e66)

## Volume Mount (File)
- Container 안에 mount path를 정의하고 path안에 configMap이 정의된 file을 mount 할 수 있음.
- 환경 변수 타입은 한번 주입하고 나면 끝이고 Pod가 재생 되어야만 변경될 수 있음.
- 하지만 Volume Mount 방식은 file이 변경될 경우 Pod의 값도 변경 됨. (1분 마다 refresh 됨.)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-mount
spec:
  containers:
  - name: container
    image: kubetm/init
    volumeMounts:
    - name: file-volume
      mountPath: /mount
  volumes:
  - name: file-volume
    configMap:
      name: cm-file
```

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/bffa1f61-2ab2-4faa-93c1-098625989e79)

<br>

# Namespace / Resource Quota / LimitRange
- Kubernetes Cluster 안에는 여러 개의 Namespace를 만들 수 있고, Namesapce에는 여러 Pod들을 생성할 수 있음.
- 각 Pod들은 Cluster 자원을 공유해서 사용하는데, 하나의 Pod가 너무 많은 자원을 사용할 경우 나머지 Pod들은 자원 사용에 문제가 생김.
- **Resource Quota**를 사용하여 Namespace마다 설정해줄 경우, Namespace마다 사용할 수 있는 최대 자원의 양을 제한할 수 있음.
- Namespace 내부에서도 마찬가지로 하나의 Pod가 자원을 너무 많이 사용할 경우를 대비해, **LimitRange** 사용하여 설정된 값보다 많은 자원을 사용하는 Pod를 Namespace에 들어오지 못하게 할 수 있음.
- **Resource Quota** / **LimitRange**는 Namespace 뿐만 아니라 Node에도 설정할 수 있음.

## Namespace
- 같은 Namespace 안에는 같은 이름의 Object를 생성할 수 없음.
- Namespace들은 각자의 자원을 타 Namespace와 공유하지 않음.
- Service는 타 Namespace의 Object의 label에는 연결하지 않음.
- Node나 Persistency Volume 등은 Namespace와 상관 없이 공용으로 사용 됨.

## 실습
```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: nm-1
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-1
  namespace: nm-1
  labels:
    app: pod
spec:
  containers:
  - name: container
    image: kubetm/app
    ports:
    - containerPort: 8080
```

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/9ea90fba-9b57-4551-9adf-13943d5fd777)

## Resource Quota
- Namespace의 자원을 설정하는 Object
- Resource Quota가 정의된 Namespace에 Pod를 생성할 때는 반드시 Pod에 requests / limits spec을 명시해야 한다. (defaultRequest 옵션 사용 시 예외 )
- 현재 Resource Quota로 설정된 자원양을 초과하는 자원 양을 사용하는 Pod는 생성 될 수 없다.
- 만약 다른 Pod가 자원을 일부 사용하고 있다면, 남은 자원 양을 초과하는 Pod는 생성 될 수 없다.
- Kubernetes 버전마다 Resource Quota로 제한할 수 있는 Object 종류가 다르므로 확인 필요.

## 실습
```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: nm-3
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: rq-1
  namespace: nm-3
spec:
  hard:
    requests.memory: 3Gi
    limits.memory: 5Gi
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-1
spec:
  containers:
  - name: container
    image: kubetm/init
    resources:
      requests:
        memory: 2Gi
      limits:
        memory: 3Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-2
  namespace: nm-3
spec:
  containers:
  - name: container
    image: kubetm/init
    resources:
      requests:
        memory: 2Gi
      limits:
        memory: 3Gi
```

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/bd5ca0f9-f3e3-4702-b966-63cfde4de3a9)

## LimitRange
- ResourceQuota는 Namespace 뿐만 아니라 Cluster 전체에 부여할 수 있는 권한이지만, LimitRange의 경우 Namespace내에서만 사용 가능함.
- maxLimitRequestRatio: request / limit의 비율 정의, 생성하는 Object의 비율이 
- defaultRequest : Objet 정의에 request / limit 을 설정하지 않을 경우, Default 값을 셋팅하는 기능.

## 실습

**1) limit range 정의**
```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: nm-5
---
apiVersion: v1
kind: LimitRange
metadata:
  name: lr-1
  namespace: nm-5
spec:
  limits:
  - type: Container
    min:
      memory: 0.1Gi
    max:
      memory: 0.5Gi
    maxLimitRequestRatio:
      memory: 1
    defaultRequest:
      memory: 0.1Gi
    default:
      memory: 0.5Gi
```
![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/bb30ab73-99a3-416c-b337-1f2d8c139027)

**2) limit range에 위배되는 pod 생성**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-1
  namespace: nm-5
spec:
  containers:
  - name: container
    image: kubetm/app
```
![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/ecaa6062-33b2-4cbf-a1b4-0e34f9152cee)

**3) limit range에 부합하는 pod 생성**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-1
  namespace: nm-5
spec:
  containers:
  - name: container
    image: kubetm/app
    resources:
      requests:
        memory: 200Mi
      limits:
        memory: 200Mi
```

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/ea10bab1-1a07-4519-acf5-f4ba246a030f)

<br>

# Controller
- **Auto Healing**: Node에 문제가 생겼을 때, 정상 Node에 Pod를 재생성하여 자동으로 서비스를 복구함.
- **Auto Scaling**: Pod에 부하가 몰릴 때, Pod를 추가 생성하여 부하를 분산.
- **SW Update**: 여러 Pod에 대한 Version을 업데이트해야할 경우, Controller를 통해 한번에 쉽게할 수 있고 업데이트 도중 문제가 생기면 rollback도 가능함.
- **Job**: 일시적인 작업을 해야하는 경우 일시적으로 Pod를 생성하여 작업을 수행하고, 수행을 마치면 Pod를 삭제함.
- Replication Controller (Deprecated) --> ReplicaSet (New)

## Template
- Controller와 Pod는 Service와 Pod처럼 Label과 Selector로 연결 됨. (아래 그림에서는 Replication이 Controller)
- 그리고 template에 Pod를 정의하게 되는데, Controller는 Pod가 죽으면 template에 정의된 내용을 사용하여 Pod를 재생성 함.
- 이를 이용하여 template을 업데이트하고, 기존 Pod를 제거하므로서 Version Update를 수동으로 할 수 있음.

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/ff71b244-d7e1-4243-94e6-edced081bcb9)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod1
  labels:
    type: web
spec:
  containers:
  - name: container
    image: kubetm/app:v1
  terminationGracePeriodSeconds: 0
```
```yaml
# Controller의 selector에 연결할 label을 넣고
# 아래 template에 정의된 Pod에도 label을 정의해야 Controller에 정상 연결 됨.
apiVersion: apps/v1
kind: ReplicationController
metadata:
  name: replica1
spec:
  replicas: 1
  selector:
    matchLabels:
      type: web
  template:
    metadata:
      name: pod1
      labels:
        type: web
    spec:
      containers:
      - name: container
        image: kubetm/app:v1
      terminationGracePeriodSeconds: 0
```

## Replicas
- replicas를 조절하여 Pod의 개수를 조절할 수 있음. (replicas를 증가시키면 scail out, 감소 시키면 scale in이라고 한다.)
- Replication과 Template을 같이 만들어서 사용할 수 있음.
 
![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/189fcf37-e40d-43c6-ad31-4cf104a37c52)

## Selector
- 위 2개의 기능과 달리, Selector는 Replicaset에만 있음.
- Replication의 selector는 key와 value가 같은 Pod에 연결 됨.
- 반면 ReplicaSet의 selector는 2가지 추가적인 속성이 있음. (matchLabels, matchExpressions)
- matchLabels은 기존과 동일하게 key와 value가 모두 같은 경우 연결 됨.
- matchExpressions은 key와 value는 좀 더 다양한 방법으로 컨트롤 할 수 있음.

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/26cf23c6-f451-45af-b4ec-2f4e0efa7325)


#### Exists / DoesNotExist / In / NotIn
- Exists: value에 상관 없이 key가 있으면 선택
- DoesNotExist: value와 상관 없이 key가 없으면 선택
- In: values라는 속성을 사용하여, 다수의 values에 있는 값들이 하나라도 있으면 선택.
- NotIn: values라는 속성을 사용하여, 다수의 values에 있는 값들이 모두 없으면 선택.

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/75d42ae5-576b-471b-8f23-81fbc4dcf7e9)

## 실습

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod1
  labels:
    type: web
spec:
  containers:
  - name: container
    image: kubetm/app:v1
  terminationGracePeriodSeconds: 0
```
```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replica1
spec:
  replicas: 1
  selector:
    matchLabels:
      type: web
  template:
    metadata:
      name: pod1
      labels:
        type: web
    spec:
      containers:
      - name: container
        image: kubetm/app:v1
      terminationGracePeriodSeconds: 0
```

**1) relicase 값 1->2로 변경 후 pod 개수 확인**

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/2b0f4049-1932-4995-b966-0016915b39d4)

**2) 수동으로 Pod 업데이트 해보기**

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/83f61eb0-acd7-45d0-a492-f0446cfac5ac)

**3) Controller 삭제 후, 연결 된 파드 모두 삭제되는 것 확인**
- 삭제 되는 것 확인할 수 없었습니다.아무래도 쿠버네티스 가비지 컬렉션이 동작하는데에 시간이 오래 걸려서 그런 것 같습니다. (--cascade=background 옵션 줘도 여전히 삭제 안됨.)

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/b72d3c31-f22f-4cbc-80e6-1fba81ab2bf9)

**4) Selector - matchLabels**
- Selector에서는 template을 정의해서 사용하기 때문에 matchExpressions를 많이 사용하지 않으므로 굳이 실습 진행하지 않음 (Node 스케줄링에서 많이 다뤄볼 예정)
- replicaset에서 selector 사용 시 주의할 점이 있는데, 아래 예시에서 template의 labels에 selector의 label이 포함되지 않을 경우 selector에서 에러를 반환 함.

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replica1
spec:
  replicas: 1
  selector:
    matchLabels:
      type: web
      ver: v3
  template:
    metadata:
      labels:
        type: web
        ver: v1
        ver: v2
        location: dev
    spec:
      containers:
      - name: container
        image: kubetm/app:v1
      terminationGracePeriodSeconds: 0
```

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/4c23f5a6-4c79-4334-8fd8-87ecbe9634b0)

## Deployment

**1) ReCreate**
- Deployment가 기존의 Pod를 중지시키고, 새로운 버전의 Pod를 생성합니다.
- Downtime이 발생하므로, 일시적인 정지가 가능한 Service에 대해 사용 가능.

**2) Rolling Update**
- Deployment가 기존의 Pod를 중지시키지 않고, 새로운 버전을 배포 함.
- 업데이트를 하는 동안 사용자들의 일부는 이전 버전에, 일부는 최신 버전에 접근하게 됨.
- 배포 중간에 추가적인 자원을 소모하는 대신 Downtime이 없다는 장점이 있음.

**3) Blue/Green**
- selector를 사용하여 Service에 연결된 Pod들을 배포 한뒤, Service의 selector만 변경하여 기존의 Pod와의 연결을 끊음으로서 새로운 버전을 배포하는 방법
- Rolling Updatge와 마찬가지로 추가적인 자원을 소모하는 대신 Downtime이 없음.
- 만약 새로 배포한 버전에 문제가 생길 경우 Selector를 다시 되돌려서 이전 버전으로 roll back이 쉽게 가능함. (문제가 없을 경우 이전 버전을 제거하면 됨)

**4) Canary**
- 카나리라는 새를 이용하여 광산의 일산화탄소량을 확인하던 방법에서 유래한 이름의 배포 방식.
- 2개의 label을 사용하여 기존의 Pod들이 동작하는 상태에서 새로운 버전의 Pod를 Service에 연결 시킨 후, 
Service를 통해 일부의 Traffic만 새로 연결한 Pod에 전달 되도록 하면 더 안전하고 적은 자원으로 새로운 버전을 테스트 할 수 있음.

## Deployment - Recreate 과정

1) Deployment 정의 시 yaml 파일에 replicaset과 동일한 값들을 정의합니다. 하지만 이 값들은 Deployment가 직접 Pod를 
만들어 관리하기 위한 것들이 아니고, Replicaset을 정의하기 위한 용도입니다. Deployment를 통해 Replicaset이 생성 되고, Label을 통해 Service를 연결 시켜서 Pod에 접근할 수 있습니다.

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/e99f4860-9f6d-432f-b4a4-774fa976984e) 

2) 이후 업데이트를 진행하게 될 경우, template에 정의된 버전을 변경해주면, Deployment가 이전 Replicaset의 replicas 값을 0으로 변경시킵니다. 
(Pod가 제거 되고, Service와의 연결도 끊겨서 Downtime이 발생하게 됩니다.) 그리고 새로운 Replicaset이 생성 되는데 여기에는 새로운 버전의 Pod가 생성됩니다. 
 
![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/7257baf9-4969-4d97-b345-f5ba83563ad7)

3) 새로운 버전의 Pod에도 동일한 Label이 있기 때문에 Service에 자동으로 연결 됩니다.

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/4fec1c68-55c4-4cde-a356-d05804784eae)

## Deployment - Recreate 실습

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-1
spec:
  selector:
    matchLabels:
      type: app
  replicas: 2
  strategy:
    type: Recreate
  revisionHistoryLimit: 1 # replicas가 0인된 replicaset을 1개만 남기겠다는 의미 (default는 10)
                          # 즉, 2번의 업데이트가 진행될 경우 가장 오래된 버전의 replicaset이 삭제 됨.
  template:
    metadata:
      labels:
        type: app
    spec:
      containers:
      - name: container
        image: kubetm/app:v1
      terminationGracePeriodSeconds: 10
```

```yaml
apiVersion: v1
kind: Service
metadata:
  name: svc-1
spec:
  selector:
    type: app
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 8080
```

1) 먼저 v1으로 배포 후 오브젝트들의 상태를 확인합니다.

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/64b2d835-00f3-48a9-bb28-805a489a3c01)

2) 1초마다 version을 확인하는 shell 명령어를 입력한 뒤, deploy의 image 버전을 변경한 후 apply 적용하여 재배포를 실행합니다.

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/0a5e60ef-f151-4c14-be84-b7f729f7d0f1)

3) 재배포 완료된 것을 확인할 수 있습니다.

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/df3e0a12-96f5-48da-a16b-051b58bfbcbc)

4) v2로 롤백해보겠습니다, 먼저 rollback 가능한 history를 확인합니다. (revisionHistoryLimit이 1이므로 2개가 나옵니다.)

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/c5eb5a3d-9fe5-49e8-8895-cd5ec85db3cf)

5) 아래의 명령어를 사용하여 rollback 가능 합니다.

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/4bd5fe4c-cbb4-4b3d-8a02-aedf68da68c5)

## Deployment - Rolling Update 과정

1) 기존의 replicaset의 replicas를 유지한채로 새로운 버전의 replicaset을 생성합니다.
![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/cb4ed266-a4a3-47c2-8d40-417e0c825fbd)

2) 이후, 기존 버전의 replicas를 하나씩 순차적으로 줄이면서 새로운 버전의 replicas를 증가시킵니다. 
마찬가지로 기존 replicaset은 삭제되지 않고 남아 있습니다. (roll back 할 수 있게)
![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/4cbed302-9fc4-438e-8c54-0fe26c0fb9e6)

3) 참고로, 새로운 replicaset이 생성 될 때 기존의 Pod와 새로운 Pod의 Label이 구분이 안되지 않을까? 하는 의문을 가질 수 있는데 
쿠버네티스에서 자체적으로 추가적인 Label을 생성하여 구분할 수 있게 하고 있습니다. (이것은 실습에서 알아봅시다.)

## Deployment - Rolling Update 실습

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-2
spec:
  selector:
    matchLabels:
      type: app2
  replicas: 2
  strategy:
    type: RollingUpdate
  minReadySeconds: 10 # 배포 간격을 10초로 둠으로서 실습간에 눈으로 보기 용이. 
  template:
    metadata:
      labels:
        type: app2
    spec:
      containers:
      - name: container
        image: kubetm/app:v1
      terminationGracePeriodSeconds: 0
```
```yaml
apiVersion: v1
kind: Service
metadata:
  name: svc-2
spec:
  selector:
    type: app2
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 8080
```

1) Rolling Update 방식으로 Deployment 배포 후 확인합니다. (v1으로 배포된 것을 확인할 수 있습니다.)

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/ea21cd38-d6fa-4aee-a681-acd7385fe87c)

2) 1초마다 version 확인하는 shell 명령어 입력 후, v1으로 yaml파일 변경 후 apply 입력하여 배포해줍니다. 
이 때 v1과 v2가 섞여서 동작하는 것을 볼 수 있고 시간이 지난 후 v1이 완전히 삭제되어 v2로 동작하는 것을 확인할 수 있습니다.

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/ec528aed-1a24-44a7-9b90-759eed52d291)

3) v2로 재배포 된 것을 확인할 수 있습니다.

![image](https://github.com/ysbaekFox/ysbaekFox.github.io/assets/54944434/def04bfb-a25c-4347-a1b5-6b6eff42c4f9)
